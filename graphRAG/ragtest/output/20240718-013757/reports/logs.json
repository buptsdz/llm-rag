{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 344, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 238, in _receive_event\n    raise RemoteProtocolError(msg)\nhttpcore.RemoteProtocolError: Server disconnected without sending a response.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1537, in _request\n    response = await self._client.send(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.RemoteProtocolError: Server disconnected without sending a response.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n    return await self._post(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n    return await self._request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1571, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n", "source": "Connection error.", "details": {"input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. When finished, output <|COMPLETE|>\n\n######################\n-Examples-\n######################\nExample 1:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \u201cIf this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\u201d\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n################\nOutput:\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\")##\n(\"entity\"<|>\"Taylor\"<|>\"person\"<|>\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\")##\n(\"entity\"<|>\"Jordan\"<|>\"person\"<|>\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\")##\n(\"entity\"<|>\"Cruz\"<|>\"person\"<|>\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\")##\n(\"entity\"<|>\"The Device\"<|>\"technology\"<|>\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\")##\n(\"relationship\"<|>\"Alex\"<|>\"Taylor\"<|>\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"<|>7)##\n(\"relationship\"<|>\"Alex\"<|>\"Jordan\"<|>\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"<|>6)##\n(\"relationship\"<|>\"Taylor\"<|>\"Jordan\"<|>\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"<|>8)##\n(\"relationship\"<|>\"Jordan\"<|>\"Cruz\"<|>\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"<|>5)##\n(\"relationship\"<|>\"Taylor\"<|>\"The Device\"<|>\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"<|>9)<|COMPLETE|>\n#############################\nExample 2:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nThey were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols\u2014it demanded a new perspective, a new resolve.\n\nTension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.\n\nTheir connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence\u2014 the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly\n#############\nOutput:\n(\"entity\"<|>\"Washington\"<|>\"location\"<|>\"Washington is a location where communications are being received, indicating its importance in the decision-making process.\")##\n(\"entity\"<|>\"Operation: Dulce\"<|>\"mission\"<|>\"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities.\")##\n(\"entity\"<|>\"The team\"<|>\"organization\"<|>\"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role.\")##\n(\"relationship\"<|>\"The team\"<|>\"Washington\"<|>\"The team receives communications from Washington, which influences their decision-making process.\"<|>7)##\n(\"relationship\"<|>\"The team\"<|>\"Operation: Dulce\"<|>\"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities.\"<|>9)<|COMPLETE|>\n#############################\nExample 3:\n\nEntity_types: [person, role, technology, organization, event, location, concept]\nText:\ntheir voice slicing through the buzz of activity. \"Control may be an illusion when facing an intelligence that literally writes its own rules,\" they stated stoically, casting a watchful eye over the flurry of data.\n\n\"It's like it's learning to communicate,\" offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. \"This gives talking to strangers' a whole new meaning.\"\n\nAlex surveyed his team\u2014each face a study in concentration, determination, and not a small measure of trepidation. \"This might well be our first contact,\" he acknowledged, \"And we need to be ready for whatever answers back.\"\n\nTogether, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable\u2014a collective introspection about their role in this grand cosmic play, one that could rewrite human history.\n\nThe encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation\n#############\nOutput:\n(\"entity\"<|>\"Sam Rivera\"<|>\"person\"<|>\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\")##\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\")##\n(\"entity\"<|>\"Control\"<|>\"concept\"<|>\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\")##\n(\"entity\"<|>\"Intelligence\"<|>\"concept\"<|>\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\")##\n(\"entity\"<|>\"First Contact\"<|>\"event\"<|>\"First Contact is the potential initial communication between humanity and an unknown intelligence.\")##\n(\"entity\"<|>\"Humanity's Response\"<|>\"event\"<|>\"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence.\")##\n(\"relationship\"<|>\"Sam Rivera\"<|>\"Intelligence\"<|>\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"<|>9)##\n(\"relationship\"<|>\"Alex\"<|>\"First Contact\"<|>\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"<|>10)##\n(\"relationship\"<|>\"Alex\"<|>\"Humanity's Response\"<|>\"Alex and his team are the key figures in Humanity's Response to the unknown intelligence.\"<|>8)##\n(\"relationship\"<|>\"Control\"<|>\"Intelligence\"<|>\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<|>7)<|COMPLETE|>\n#############################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: face.\n\nMarley's face. It was not in impenetrable shadow, as the other objects\nin the yard were, but had a dismal light about it, like a bad lobster in\na dark cellar. It was not angry or ferocious, but looked at Scrooge as\nMarley used to look; with ghostly spectacles turned up on its ghostly\nforehead. The hair was curiously stirred, as if by breath or hot air;\nand, though the eyes were wide open, they were perfectly motionless.\nThat, and its livid colour, made it horrible; but its horror seemed to\nbe in spite of the face, and beyond its control, rather than a part of\nits own expression.\n\nAs Scrooge looked fixedly at this phenomenon, it was a knocker again.\n\nTo say that he was not startled, or that his blood was not conscious of\na terrible sensation to which it had been a stranger from infancy, would\nbe untrue. But he put his hand upon the key he had relinquished, turned\nit sturdily, walked in, and lighted his candle.\n\nHe _did_ pause, with a moment's irresolution, before he shut the door;\nand he _did_ look cautiously behind it first, as if he half expected to\nbe terrified with the sight of Marley's pigtail sticking out into the\nhall. But there was nothing on the back of the door,\n######################\nOutput:"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 344, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 238, in _receive_event\n    raise RemoteProtocolError(msg)\nhttpcore.RemoteProtocolError: Server disconnected without sending a response.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1537, in _request\n    response = await self._client.send(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.RemoteProtocolError: Server disconnected without sending a response.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n    return await self._post(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n    return await self._request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1571, in _request\n    raise APIConnectionError(request=request) from err\nopenai.APIConnectionError: Connection error.\n", "source": "Connection error.", "details": {"input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. When finished, output <|COMPLETE|>\n\n######################\n-Examples-\n######################\nExample 1:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \u201cIf this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\u201d\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n################\nOutput:\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\")##\n(\"entity\"<|>\"Taylor\"<|>\"person\"<|>\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\")##\n(\"entity\"<|>\"Jordan\"<|>\"person\"<|>\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\")##\n(\"entity\"<|>\"Cruz\"<|>\"person\"<|>\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\")##\n(\"entity\"<|>\"The Device\"<|>\"technology\"<|>\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\")##\n(\"relationship\"<|>\"Alex\"<|>\"Taylor\"<|>\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"<|>7)##\n(\"relationship\"<|>\"Alex\"<|>\"Jordan\"<|>\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"<|>6)##\n(\"relationship\"<|>\"Taylor\"<|>\"Jordan\"<|>\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"<|>8)##\n(\"relationship\"<|>\"Jordan\"<|>\"Cruz\"<|>\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"<|>5)##\n(\"relationship\"<|>\"Taylor\"<|>\"The Device\"<|>\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"<|>9)<|COMPLETE|>\n#############################\nExample 2:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nThey were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols\u2014it demanded a new perspective, a new resolve.\n\nTension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.\n\nTheir connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence\u2014 the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly\n#############\nOutput:\n(\"entity\"<|>\"Washington\"<|>\"location\"<|>\"Washington is a location where communications are being received, indicating its importance in the decision-making process.\")##\n(\"entity\"<|>\"Operation: Dulce\"<|>\"mission\"<|>\"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities.\")##\n(\"entity\"<|>\"The team\"<|>\"organization\"<|>\"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role.\")##\n(\"relationship\"<|>\"The team\"<|>\"Washington\"<|>\"The team receives communications from Washington, which influences their decision-making process.\"<|>7)##\n(\"relationship\"<|>\"The team\"<|>\"Operation: Dulce\"<|>\"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities.\"<|>9)<|COMPLETE|>\n#############################\nExample 3:\n\nEntity_types: [person, role, technology, organization, event, location, concept]\nText:\ntheir voice slicing through the buzz of activity. \"Control may be an illusion when facing an intelligence that literally writes its own rules,\" they stated stoically, casting a watchful eye over the flurry of data.\n\n\"It's like it's learning to communicate,\" offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. \"This gives talking to strangers' a whole new meaning.\"\n\nAlex surveyed his team\u2014each face a study in concentration, determination, and not a small measure of trepidation. \"This might well be our first contact,\" he acknowledged, \"And we need to be ready for whatever answers back.\"\n\nTogether, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable\u2014a collective introspection about their role in this grand cosmic play, one that could rewrite human history.\n\nThe encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation\n#############\nOutput:\n(\"entity\"<|>\"Sam Rivera\"<|>\"person\"<|>\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\")##\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\")##\n(\"entity\"<|>\"Control\"<|>\"concept\"<|>\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\")##\n(\"entity\"<|>\"Intelligence\"<|>\"concept\"<|>\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\")##\n(\"entity\"<|>\"First Contact\"<|>\"event\"<|>\"First Contact is the potential initial communication between humanity and an unknown intelligence.\")##\n(\"entity\"<|>\"Humanity's Response\"<|>\"event\"<|>\"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence.\")##\n(\"relationship\"<|>\"Sam Rivera\"<|>\"Intelligence\"<|>\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"<|>9)##\n(\"relationship\"<|>\"Alex\"<|>\"First Contact\"<|>\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"<|>10)##\n(\"relationship\"<|>\"Alex\"<|>\"Humanity's Response\"<|>\"Alex and his team are the key figures in Humanity's Response to the unknown intelligence.\"<|>8)##\n(\"relationship\"<|>\"Control\"<|>\"Intelligence\"<|>\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<|>7)<|COMPLETE|>\n#############################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: ,' he said impatiently.\n\n'Your own feeling tells you that you were not what you are,' she\nreturned. 'I am. That which promised happiness when we were one in heart\nis fraught with misery now that we are two. How often and how keenly I\nhave thought of this I will not say. It is enough that I _have_ thought\nof it, and can release you.'\n\n'Have I ever sought release?'\n\n'In words. No. Never.'\n\n'In what, then?'\n\n'In a changed nature; in an altered spirit; in another atmosphere of\nlife; another Hope as its great end. In everything that made my love of\nany worth or value in your sight. If this had never been between us,'\nsaid the girl, looking mildly, but with steadiness, upon him; 'tell me,\nwould you seek me out and try to win me now? Ah, no!'\n\nHe seemed to yield to the justice of this supposition in spite of\nhimself. But he said, with a struggle, 'You think not.'\n\n'I would gladly think otherwise if I could,' she answered. 'Heaven\nknows! When _I_ have learned a Truth like this, I know how strong and\nirresistible it must be. But if you were free to-day, to-morrow,\nyesterday, can even I believe that you would choose a dowerless\ngirl--you who, in\n######################\nOutput:"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 344, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1537, in _request\n    response = await self._client.send(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n    return await self._post(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n    return await self._request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1556, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. When finished, output <|COMPLETE|>\n\n######################\n-Examples-\n######################\nExample 1:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \u201cIf this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\u201d\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n################\nOutput:\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\")##\n(\"entity\"<|>\"Taylor\"<|>\"person\"<|>\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\")##\n(\"entity\"<|>\"Jordan\"<|>\"person\"<|>\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\")##\n(\"entity\"<|>\"Cruz\"<|>\"person\"<|>\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\")##\n(\"entity\"<|>\"The Device\"<|>\"technology\"<|>\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\")##\n(\"relationship\"<|>\"Alex\"<|>\"Taylor\"<|>\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"<|>7)##\n(\"relationship\"<|>\"Alex\"<|>\"Jordan\"<|>\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"<|>6)##\n(\"relationship\"<|>\"Taylor\"<|>\"Jordan\"<|>\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"<|>8)##\n(\"relationship\"<|>\"Jordan\"<|>\"Cruz\"<|>\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"<|>5)##\n(\"relationship\"<|>\"Taylor\"<|>\"The Device\"<|>\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"<|>9)<|COMPLETE|>\n#############################\nExample 2:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nThey were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols\u2014it demanded a new perspective, a new resolve.\n\nTension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.\n\nTheir connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence\u2014 the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly\n#############\nOutput:\n(\"entity\"<|>\"Washington\"<|>\"location\"<|>\"Washington is a location where communications are being received, indicating its importance in the decision-making process.\")##\n(\"entity\"<|>\"Operation: Dulce\"<|>\"mission\"<|>\"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities.\")##\n(\"entity\"<|>\"The team\"<|>\"organization\"<|>\"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role.\")##\n(\"relationship\"<|>\"The team\"<|>\"Washington\"<|>\"The team receives communications from Washington, which influences their decision-making process.\"<|>7)##\n(\"relationship\"<|>\"The team\"<|>\"Operation: Dulce\"<|>\"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities.\"<|>9)<|COMPLETE|>\n#############################\nExample 3:\n\nEntity_types: [person, role, technology, organization, event, location, concept]\nText:\ntheir voice slicing through the buzz of activity. \"Control may be an illusion when facing an intelligence that literally writes its own rules,\" they stated stoically, casting a watchful eye over the flurry of data.\n\n\"It's like it's learning to communicate,\" offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. \"This gives talking to strangers' a whole new meaning.\"\n\nAlex surveyed his team\u2014each face a study in concentration, determination, and not a small measure of trepidation. \"This might well be our first contact,\" he acknowledged, \"And we need to be ready for whatever answers back.\"\n\nTogether, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable\u2014a collective introspection about their role in this grand cosmic play, one that could rewrite human history.\n\nThe encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation\n#############\nOutput:\n(\"entity\"<|>\"Sam Rivera\"<|>\"person\"<|>\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\")##\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\")##\n(\"entity\"<|>\"Control\"<|>\"concept\"<|>\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\")##\n(\"entity\"<|>\"Intelligence\"<|>\"concept\"<|>\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\")##\n(\"entity\"<|>\"First Contact\"<|>\"event\"<|>\"First Contact is the potential initial communication between humanity and an unknown intelligence.\")##\n(\"entity\"<|>\"Humanity's Response\"<|>\"event\"<|>\"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence.\")##\n(\"relationship\"<|>\"Sam Rivera\"<|>\"Intelligence\"<|>\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"<|>9)##\n(\"relationship\"<|>\"Alex\"<|>\"First Contact\"<|>\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"<|>10)##\n(\"relationship\"<|>\"Alex\"<|>\"Humanity's Response\"<|>\"Alex and his team are the key figures in Humanity's Response to the unknown intelligence.\"<|>8)##\n(\"relationship\"<|>\"Control\"<|>\"Intelligence\"<|>\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<|>7)<|COMPLETE|>\n#############################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: Replacement or Refund\u201d described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg\u2122 trademark, and any other party distributing a Project\nGutenberg\u2122 electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from. If you\nreceived the work on a physical medium, you must return the medium\nwith your written explanation. The person or entity that provided you\nwith the defective work may elect to provide a replacement copy in\nlieu of a refund. If you received the\n######################\nOutput:"}}
{"type": "error", "data": "Error Invoking LLM", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 373, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 216, in handle_async_request\n    raise exc from None\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 196, in handle_async_request\n    response = await connection.handle_async_request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http_proxy.py\", line 344, in handle_async_request\n    return await self._connection.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 143, in handle_async_request\n    raise exc\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 113, in handle_async_request\n    ) = await self._receive_response_headers(**kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 186, in _receive_response_headers\n    event = await self._receive_event(timeout=timeout)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_async\\http11.py\", line 224, in _receive_event\n    data = await self._network_stream.read(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 32, in read\n    with map_exceptions(exc_map):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1537, in _request\n    response = await self._client.send(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1661, in send\n    response = await self._send_handling_auth(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1689, in _send_handling_auth\n    response = await self._send_handling_redirects(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1726, in _send_handling_redirects\n    response = await self._send_single_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_client.py\", line 1763, in _send_single_request\n    response = await transport.handle_async_request(request)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 372, in handle_async_request\n    with map_httpcore_exceptions():\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\httpx\\_transports\\default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 53, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 55, in _execute_llm\n    completion = await self.client.chat.completions.create(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n    return await self._post(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n    return await self._request(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\openai\\_base_client.py\", line 1556, in _request\n    raise APITimeoutError(request=request) from err\nopenai.APITimeoutError: Request timed out.\n", "source": "Request timed out.", "details": {"input": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>\n\n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n\n4. When finished, output <|COMPLETE|>\n\n######################\n-Examples-\n######################\nExample 1:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \u201cIf this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\u201d\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n################\nOutput:\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is a character who experiences frustration and is observant of the dynamics among other characters.\")##\n(\"entity\"<|>\"Taylor\"<|>\"person\"<|>\"Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\")##\n(\"entity\"<|>\"Jordan\"<|>\"person\"<|>\"Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\")##\n(\"entity\"<|>\"Cruz\"<|>\"person\"<|>\"Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\")##\n(\"entity\"<|>\"The Device\"<|>\"technology\"<|>\"The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\")##\n(\"relationship\"<|>\"Alex\"<|>\"Taylor\"<|>\"Alex is affected by Taylor's authoritarian certainty and observes changes in Taylor's attitude towards the device.\"<|>7)##\n(\"relationship\"<|>\"Alex\"<|>\"Jordan\"<|>\"Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.\"<|>6)##\n(\"relationship\"<|>\"Taylor\"<|>\"Jordan\"<|>\"Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\"<|>8)##\n(\"relationship\"<|>\"Jordan\"<|>\"Cruz\"<|>\"Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\"<|>5)##\n(\"relationship\"<|>\"Taylor\"<|>\"The Device\"<|>\"Taylor shows reverence towards the device, indicating its importance and potential impact.\"<|>9)<|COMPLETE|>\n#############################\nExample 2:\n\nEntity_types: [person, technology, mission, organization, location]\nText:\nThey were no longer mere operatives; they had become guardians of a threshold, keepers of a message from a realm beyond stars and stripes. This elevation in their mission could not be shackled by regulations and established protocols\u2014it demanded a new perspective, a new resolve.\n\nTension threaded through the dialogue of beeps and static as communications with Washington buzzed in the background. The team stood, a portentous air enveloping them. It was clear that the decisions they made in the ensuing hours could redefine humanity's place in the cosmos or condemn them to ignorance and potential peril.\n\nTheir connection to the stars solidified, the group moved to address the crystallizing warning, shifting from passive recipients to active participants. Mercer's latter instincts gained precedence\u2014 the team's mandate had evolved, no longer solely to observe and report but to interact and prepare. A metamorphosis had begun, and Operation: Dulce hummed with the newfound frequency of their daring, a tone set not by the earthly\n#############\nOutput:\n(\"entity\"<|>\"Washington\"<|>\"location\"<|>\"Washington is a location where communications are being received, indicating its importance in the decision-making process.\")##\n(\"entity\"<|>\"Operation: Dulce\"<|>\"mission\"<|>\"Operation: Dulce is described as a mission that has evolved to interact and prepare, indicating a significant shift in objectives and activities.\")##\n(\"entity\"<|>\"The team\"<|>\"organization\"<|>\"The team is portrayed as a group of individuals who have transitioned from passive observers to active participants in a mission, showing a dynamic change in their role.\")##\n(\"relationship\"<|>\"The team\"<|>\"Washington\"<|>\"The team receives communications from Washington, which influences their decision-making process.\"<|>7)##\n(\"relationship\"<|>\"The team\"<|>\"Operation: Dulce\"<|>\"The team is directly involved in Operation: Dulce, executing its evolved objectives and activities.\"<|>9)<|COMPLETE|>\n#############################\nExample 3:\n\nEntity_types: [person, role, technology, organization, event, location, concept]\nText:\ntheir voice slicing through the buzz of activity. \"Control may be an illusion when facing an intelligence that literally writes its own rules,\" they stated stoically, casting a watchful eye over the flurry of data.\n\n\"It's like it's learning to communicate,\" offered Sam Rivera from a nearby interface, their youthful energy boding a mix of awe and anxiety. \"This gives talking to strangers' a whole new meaning.\"\n\nAlex surveyed his team\u2014each face a study in concentration, determination, and not a small measure of trepidation. \"This might well be our first contact,\" he acknowledged, \"And we need to be ready for whatever answers back.\"\n\nTogether, they stood on the edge of the unknown, forging humanity's response to a message from the heavens. The ensuing silence was palpable\u2014a collective introspection about their role in this grand cosmic play, one that could rewrite human history.\n\nThe encrypted dialogue continued to unfold, its intricate patterns showing an almost uncanny anticipation\n#############\nOutput:\n(\"entity\"<|>\"Sam Rivera\"<|>\"person\"<|>\"Sam Rivera is a member of a team working on communicating with an unknown intelligence, showing a mix of awe and anxiety.\")##\n(\"entity\"<|>\"Alex\"<|>\"person\"<|>\"Alex is the leader of a team attempting first contact with an unknown intelligence, acknowledging the significance of their task.\")##\n(\"entity\"<|>\"Control\"<|>\"concept\"<|>\"Control refers to the ability to manage or govern, which is challenged by an intelligence that writes its own rules.\")##\n(\"entity\"<|>\"Intelligence\"<|>\"concept\"<|>\"Intelligence here refers to an unknown entity capable of writing its own rules and learning to communicate.\")##\n(\"entity\"<|>\"First Contact\"<|>\"event\"<|>\"First Contact is the potential initial communication between humanity and an unknown intelligence.\")##\n(\"entity\"<|>\"Humanity's Response\"<|>\"event\"<|>\"Humanity's Response is the collective action taken by Alex's team in response to a message from an unknown intelligence.\")##\n(\"relationship\"<|>\"Sam Rivera\"<|>\"Intelligence\"<|>\"Sam Rivera is directly involved in the process of learning to communicate with the unknown intelligence.\"<|>9)##\n(\"relationship\"<|>\"Alex\"<|>\"First Contact\"<|>\"Alex leads the team that might be making the First Contact with the unknown intelligence.\"<|>10)##\n(\"relationship\"<|>\"Alex\"<|>\"Humanity's Response\"<|>\"Alex and his team are the key figures in Humanity's Response to the unknown intelligence.\"<|>8)##\n(\"relationship\"<|>\"Control\"<|>\"Intelligence\"<|>\"The concept of Control is challenged by the Intelligence that writes its own rules.\"<|>7)<|COMPLETE|>\n#############################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: Replacement or Refund\u201d described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg\u2122 trademark, and any other party distributing a Project\nGutenberg\u2122 electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from. If you\nreceived the work on a physical medium, you must return the medium\nwith your written explanation. The person or entity that provided you\nwith the defective work may elect to provide a replacement copy in\nlieu of a refund. If you received the\n######################\nOutput:"}}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Community Report Extraction Error", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\graph\\extractors\\community_reports\\community_reports_extractor.py\", line 58, in __call__\n    await self._llm(\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\json_parsing_llm.py\", line 34, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_token_replacing_llm.py\", line 37, in __call__\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_history_tracking_llm.py\", line 33, in __call__\n    output = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\caching_llm.py\", line 104, in __call__\n    result = await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n    return self.__get_result()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\base\\base_llm.py\", line 48, in __call__\n    return await self._invoke_json(input, **kwargs)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 82, in _invoke_json\n    result = await generate()\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 74, in generate\n    await self._native_json(input, **{**kwargs, \"name\": call_name})\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\openai_chat_llm.py\", line 108, in _native_json\n    json_output = try_parse_json_object(raw_output)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\llm\\openai\\utils.py\", line 93, in try_parse_json_object\n    result = json.loads(input)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\__init__.py\", line 346, in loads\n    return _default_decoder.decode(s)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\json\\decoder.py\", line 355, in raw_decode\n    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\njson.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n", "source": "Expecting value: line 1 column 1 (char 0)", "details": null}
{"type": "error", "data": "Error executing verb \"window\" in create_final_community_reports: 'community'", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 410, in _execute_verb\n    result = node.verb.func(**verb_args)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\datashaper\\engine\\verbs\\window.py\", line 73, in window\n    window = __window_function_map[window_operation](input_table[column])\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\pandas\\core\\frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\pandas\\core\\indexes\\range.py\", line 417, in get_loc\n    raise KeyError(key)\nKeyError: 'community'\n", "source": "'community'", "details": null}
{"type": "error", "data": "Error running pipeline!", "stack": "Traceback (most recent call last):\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\graphrag\\index\\run.py\", line 323, in run_pipeline\n    result = await workflow.run(context, callbacks)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 369, in run\n    timing = await self._execute_verb(node, context, callbacks)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 410, in _execute_verb\n    result = node.verb.func(**verb_args)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\datashaper\\engine\\verbs\\window.py\", line 73, in window\n    window = __window_function_map[window_operation](input_table[column])\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\pandas\\core\\frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"E:\\Anaconda\\system_file\\envs\\llm-rag\\lib\\site-packages\\pandas\\core\\indexes\\range.py\", line 417, in get_loc\n    raise KeyError(key)\nKeyError: 'community'\n", "source": "'community'", "details": null}
